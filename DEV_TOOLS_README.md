# ğŸ› ï¸ Herramientas de Desarrollo PIMST

Este documento describe todas las herramientas disponibles para el desarrollo, testing y benchmarking de PIMST.

## ğŸ“‹ Tabla de Contenidos

- [Scripts de Testing](#scripts-de-testing)
- [Scripts de Benchmarking](#scripts-de-benchmarking)
- [Scripts de ComparaciÃ³n](#scripts-de-comparaciÃ³n)
- [GestiÃ³n de Versiones](#gestiÃ³n-de-versiones)
- [Rastreo de Rendimiento](#rastreo-de-rendimiento)
- [Workflows Recomendados](#workflows-recomendados)

---

## ğŸ§ª Scripts de Testing

### `quick_test.sh` - Tests RÃ¡pidos Pre-Commit

**PropÃ³sito**: Ejecutar tests rÃ¡pidos antes de hacer commit para asegurar calidad.

**Uso**:
```bash
./quick_test.sh
```

**QuÃ© hace**:
- âœ… Tests unitarios con pytest
- âœ… Code coverage (>70% requerido)
- âœ… Type checking con mypy (si estÃ¡ instalado)
- âœ… Linting con flake8 (si estÃ¡ instalado)
- âœ… Test de rendimiento rÃ¡pido (N=30, N=50)

**Tiempo**: ~1-2 minutos

**CuÃ¡ndo usar**: Antes de cada commit importante

---

## ğŸƒ Scripts de Benchmarking

### `benchmark_suite.sh` - Suite Completa de Benchmarks

**PropÃ³sito**: Ejecutar benchmarks completos con menÃº interactivo.

**Uso**:
```bash
./benchmark_suite.sh
```

**Opciones disponibles**:
1. **Quick Test** (1 min) - Tests unitarios rÃ¡pidos
2. **Small Benchmark** (5-10 min) - Instancias Nâ‰¤100 vs OR-Tools
3. **Large Benchmark** (20-40 min) - Instancias N=200-1000
4. **Market Compare** (30-60 min) - ComparaciÃ³n completa con el mercado
5. **Full Suite** (60-120 min) - Todos los benchmarks
6. **Custom** - SelecciÃ³n personalizada

**Output**: Crea carpeta `benchmark_results/session_TIMESTAMP/` con todos los resultados.

**CaracterÃ­sticas**:
- ğŸ“Š MenÃº interactivo
- ğŸ“ OrganizaciÃ³n automÃ¡tica de resultados
- ğŸ“ Logs detallados de cada ejecuciÃ³n
- ğŸ“ˆ GeneraciÃ³n de reporte resumen
- ğŸ¨ Colores en terminal para mejor visualizaciÃ³n

---

### `benchmark_comparison.py` - Benchmark vs OR-Tools

**PropÃ³sito**: ComparaciÃ³n detallada con Google OR-Tools en instancias pequeÃ±as/medianas.

**Uso**:
```bash
python benchmark_comparison.py
```

**QuÃ© hace**:
- Genera 11 datasets diversos (random, clustered, grid, circle)
- Compara PIMST (fast, balanced, optimal) vs OR-Tools
- Calcula gaps, speedups y estadÃ­sticas
- Guarda resultados en JSON y genera BENCHMARK_SUMMARY.md

**Output**:
- `benchmark_results.json` - Resultados detallados
- `BENCHMARK_SUMMARY.md` - Resumen en Markdown

**Tiempo**: 5-10 minutos

---

### `benchmark_large_scale.py` - Benchmark de Gran Escala

**PropÃ³sito**: Testing en instancias grandes (N=200-1000).

**Uso**:
```bash
python benchmark_large_scale.py
```

**QuÃ© hace**:
- Genera instancias de 200, 500 y 1000 ciudades
- Prueba diferentes tipos (random, clustered, grid, circle)
- Analiza escalabilidad y complejidad temporal
- Compara con resultados publicados de LKH

**Output**:
- `large_benchmark_results.json`

**Tiempo**: 20-40 minutos

---

### `compare_with_market.py` - ComparaciÃ³n Completa con el Mercado

**PropÃ³sito**: Comparar PIMST con TODOS los solvers disponibles.

**Uso**:
```bash
python compare_with_market.py
```

**QuÃ© compara**:
- Google OR-Tools (siempre)
- Python-TSP exact y SA (si estÃ¡ instalado)
- LKH-3 (si estÃ¡ compilado)

**QuÃ© hace**:
- Detecta automÃ¡ticamente solvers disponibles
- Ejecuta comparaciÃ³n completa
- Genera estadÃ­sticas detalladas
- Guarda resultados timestamped

**Output**:
- `comparison_results/market_comparison_TIMESTAMP.json`

**Tiempo**: 30-60 minutos

---

## ğŸ” Scripts de ComparaciÃ³n

### `compare_versions.py` - Comparar Dos Versiones

**PropÃ³sito**: Comparar resultados de benchmark entre dos versiones.

**Uso bÃ¡sico**:
```bash
# Comparar dos archivos
python compare_versions.py v0.21.0_results.json v0.22.0_results.json

# Comparar todos los archivos en un directorio
python compare_versions.py --dir benchmark_history/
```

**QuÃ© hace**:
- Calcula cambios en calidad (gap %)
- Calcula cambios en tiempo de ejecuciÃ³n
- Genera estadÃ­sticas (promedio, mediana, min, max)
- Provee veredicto automÃ¡tico (mejora/regresiÃ³n/similar)
- Da recomendaciones sobre si hacer merge

**Output**: Tabla comparativa en terminal

**Ejemplo de output**:
```
Instancia              N     Î” Calidad    Î” Tiempo     Estado
-----------------------------------------------------------
random-50             50     -2.15%       +5.2%        âœ…
grid-100             100      ~           -10.1%       =
```

---

### `compare_two_versions.sh` - Comparar Versiones Git

**PropÃ³sito**: Comparar automÃ¡ticamente dos tags/commits de git.

**Uso**:
```bash
./compare_two_versions.sh v0.21.0 v0.22.0
```

**QuÃ© hace**:
1. Checkout de versiÃ³n 1
2. Instala y ejecuta benchmark
3. Guarda resultados
4. Checkout de versiÃ³n 2
5. Instala y ejecuta benchmark
6. Compara ambos resultados
7. Restaura rama original

**Tiempo**: 10-20 minutos

---

### `compare_with_main.sh` - Comparar con Main

**PropÃ³sito**: Comparar tu rama actual con main antes de hacer merge.

**Uso**:
```bash
./compare_with_main.sh
```

**QuÃ© hace**:
1. Ejecuta benchmark en tu rama actual
2. Cambia temporalmente a main
3. Ejecuta benchmark en main
4. Compara resultados
5. Vuelve a tu rama

**Tiempo**: 10-20 minutos

**CuÃ¡ndo usar**: Antes de abrir Pull Request

---

## ğŸ“¦ GestiÃ³n de Versiones

### `version_manager.py` - Gestor de Versiones

**PropÃ³sito**: Actualizar versiÃ³n del proyecto automÃ¡ticamente en todos los archivos.

**Uso**:
```bash
# Ver versiÃ³n actual
python version_manager.py --show

# Incrementar patch (0.22.0 â†’ 0.22.1)
python version_manager.py --bump patch

# Incrementar minor (0.22.0 â†’ 0.23.0)
python version_manager.py --bump minor

# Incrementar major (0.22.0 â†’ 1.0.0)
python version_manager.py --bump major

# Establecer versiÃ³n especÃ­fica
python version_manager.py --set 1.0.0
```

**QuÃ© actualiza**:
- `src/pimst/__init__.py`
- `setup.py`
- `README.md`
- `CHANGELOG.md` (crea nueva entrada)

**Output**: Comandos git sugeridos para commit y tag

---

### `CHANGELOG.md` - Historial de Cambios

**PropÃ³sito**: Documentar todos los cambios del proyecto.

**Formato**: [Keep a Changelog](https://keepachangelog.com/)

**CategorÃ­as**:
- **AÃ±adido**: Nuevas caracterÃ­sticas
- **Mejorado**: Mejoras en funcionalidades existentes
- **Corregido**: CorrecciÃ³n de bugs
- **Obsoleto**: CaracterÃ­sticas que serÃ¡n eliminadas
- **Eliminado**: CaracterÃ­sticas eliminadas
- **Seguridad**: Vulnerabilidades corregidas

**CuÃ¡ndo actualizar**: Con cada cambio significativo

---

## ğŸ“ˆ Rastreo de Rendimiento

### `performance_tracker.py` - Rastreador de Rendimiento

**PropÃ³sito**: Mantener historial de rendimiento a lo largo del tiempo.

**Uso**:
```bash
# AÃ±adir benchmark al historial
python performance_tracker.py --add benchmark_results.json --notes "Mejora en gravity-guided"

# Listar todos los benchmarks
python performance_tracker.py --list

# Generar reporte
python performance_tracker.py --report

# Generar grÃ¡ficos
python performance_tracker.py --plot
```

**QuÃ© hace**:
- Almacena resultados en SQLite (`performance_history.db`)
- Asocia resultados con commit hash y versiÃ³n
- Genera grÃ¡ficos de evoluciÃ³n temporal
- Compara automÃ¡ticamente con versiÃ³n anterior

**Output**:
- `performance_history.db` - Base de datos SQLite
- `performance_history.png` - GrÃ¡fico de evoluciÃ³n
- `performance_by_size.png` - GrÃ¡fico por tamaÃ±o de instancia

---

## ğŸš€ Workflows Recomendados

### 1. Workflow Diario (Desarrollo)

```bash
# Antes de empezar a trabajar
git pull origin main

# DespuÃ©s de hacer cambios
./quick_test.sh

# Si los tests pasan
git add .
git commit -m "feat: tu mensaje"
git push
```

---

### 2. Workflow Pre-Commit (Cambios Importantes)

```bash
# 1. Tests rÃ¡pidos
./quick_test.sh

# 2. Si pasan, benchmark rÃ¡pido
python benchmark_comparison.py --quick

# 3. Comparar con main
./compare_with_main.sh

# 4. Si todo estÃ¡ bien, commit
git add .
git commit -m "feat: descripciÃ³n del cambio"

# 5. AÃ±adir al historial
python performance_tracker.py --add benchmark_results.json --notes "DescripciÃ³n"

# 6. Push
git push
```

---

### 3. Workflow Pre-Release

```bash
# 1. Tests completos
pytest tests/ -v --cov=pimst

# 2. Benchmarks completos
./benchmark_suite.sh
# Seleccionar opciÃ³n 5 (Full Suite)

# 3. Actualizar versiÃ³n
python version_manager.py --bump minor

# 4. Actualizar CHANGELOG manualmente
nano CHANGELOG.md

# 5. AÃ±adir al historial
python performance_tracker.py --add benchmark_results.json --notes "Release v0.23.0"

# 6. Generar grÃ¡ficos
python performance_tracker.py --plot

# 7. Actualizar README con resultados

# 8. Commit todo
git add .
git commit -m "chore: Release v0.23.0"

# 9. Crear tag
git tag -a v0.23.0 -m "Release v0.23.0: DescripciÃ³n"

# 10. Push con tags
git push origin main
git push origin v0.23.0
```

---

### 4. Workflow de InvestigaciÃ³n (Probar Nueva Idea)

```bash
# 1. Crear rama
git checkout -b experiment/new-algorithm

# 2. Implementar cambios
# ... editar cÃ³digo ...

# 3. Tests bÃ¡sicos
./quick_test.sh

# 4. Benchmark
python benchmark_comparison.py

# 5. Comparar con main
./compare_with_main.sh

# 6. Si mejora, guardar resultados
COMMIT=$(git rev-parse --short HEAD)
cp benchmark_results.json experiments/${COMMIT}_new_algorithm.json

# 7. Si no mejora, descartar o seguir iterando
git checkout main
git branch -D experiment/new-algorithm
```

---

### 5. Workflow de Paper (Preparar PublicaciÃ³n)

```bash
# 1. Suite completa de benchmarks
./benchmark_suite.sh
# Seleccionar opciÃ³n 5

# 2. ComparaciÃ³n con mercado
python compare_with_market.py

# 3. Generar todos los grÃ¡ficos
python performance_tracker.py --plot

# 4. Copiar resultados a paper/
mkdir -p paper/results
cp benchmark_results/*.json paper/results/
cp *.png paper/figures/

# 5. Generar tablas en LaTeX
python generate_latex_tables.py

# 6. Crear release en GitHub con resultados
git tag -a paper-v1.0 -m "VersiÃ³n para paper"
git push --tags
```

---

## ğŸ¯ Alias Ãštiles

AÃ±ade estos alias a tu `~/.bashrc` o `~/.bash_profile`:

```bash
# PIMST Development Aliases
alias ptest='./quick_test.sh'
alias pbench='./benchmark_suite.sh'
alias pcompare='./compare_with_main.sh'
alias pversion='python version_tracker.py --show'
alias ptrack='python performance_tracker.py'

# Quick benchmark function
pbench-quick() {
    python benchmark_comparison.py
    python performance_tracker.py --add benchmark_results.json --notes "$1"
    echo "âœ… Benchmark guardado en historial"
}

# Compare two versions function
pcompare-versions() {
    ./compare_two_versions.sh $1 $2
}
```

Uso:
```bash
ptest                              # Tests rÃ¡pidos
pbench                             # Benchmark suite
pcompare                           # Comparar con main
pbench-quick "Mi nota"            # Benchmark + guardar en historial
pcompare-versions v0.21.0 v0.22.0 # Comparar dos versiones
```

---

## ğŸ“Š Estructura de Archivos Recomendada

```
pimst-solver/
â”œâ”€â”€ benchmark_results/           # Resultados organizados por sesiÃ³n
â”‚   â”œâ”€â”€ session_20251105_143022/
â”‚   â”‚   â”œâ”€â”€ session_info.txt
â”‚   â”‚   â”œâ”€â”€ small_benchmark.json
â”‚   â”‚   â”œâ”€â”€ large_benchmark.json
â”‚   â”‚   â””â”€â”€ SUMMARY_REPORT.md
â”‚   â””â”€â”€ session_20251106_091533/
â”‚       â””â”€â”€ ...
â”œâ”€â”€ benchmark_history/           # Resultados histÃ³ricos para comparaciÃ³n
â”‚   â”œâ”€â”€ v0.21.0_results.json
â”‚   â”œâ”€â”€ v0.22.0_results.json
â”‚   â””â”€â”€ abc123f_experiment.json
â”œâ”€â”€ experiments/                 # Experimentos y tests de nuevas ideas
â”‚   â”œâ”€â”€ gravity_v2_results.json
â”‚   â””â”€â”€ multistart_comparison.json
â”œâ”€â”€ performance_history.db       # Base de datos de rendimiento
â”œâ”€â”€ performance_history.png      # GrÃ¡ficos de evoluciÃ³n
â””â”€â”€ paper/                       # Materiales para publicaciÃ³n
    â”œâ”€â”€ results/
    â”œâ”€â”€ figures/
    â””â”€â”€ tables/
```

---

## ğŸ› Troubleshooting

### Problema: Scripts bash no se ejecutan

**SoluciÃ³n**:
```bash
chmod +x *.sh
```

### Problema: Import error en Python scripts

**SoluciÃ³n**:
```bash
pip install -e .
```

### Problema: Benchmarks muy lentos

**SoluciÃ³n**: Usar versiÃ³n quick o reducir instancias:
```bash
python benchmark_comparison.py --max-size 50
```

### Problema: Git hooks no funcionan

**SoluciÃ³n**:
```bash
chmod +x .git/hooks/*
```

---

## ğŸ“š DocumentaciÃ³n Adicional

- [GIT_COMPARISON_GUIDE.md](GIT_COMPARISON_GUIDE.md) - GuÃ­a completa de uso de git bash
- [CONTRIBUTING.md](CONTRIBUTING.md) - GuÃ­a de contribuciÃ³n
- [CHANGELOG.md](CHANGELOG.md) - Historial de cambios

---

## ğŸ¤ Contribuir

Â¿Tienes ideas para mejorar las herramientas de desarrollo? 

1. Crea un issue describiendo tu propuesta
2. Fork el repositorio
3. Implementa tu mejora
4. Abre un Pull Request

---

**Â¡Feliz desarrollo con PIMST! ğŸš€**
